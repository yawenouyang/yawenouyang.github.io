<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="en">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="cnn,neuralNetwork," />










<meta name="description" content="本文是阅读魏秀参发布的CNN的记录与思考。 绪论深度学习机器学习：人工智能的一个分支，致力于如何通过计算的手段，利用经验(experience)改善计算系统自身的性能，这里经验往往对应以特征(feature)的形式存储的数据(data)，传统机器学习算法便是通过这些数据产生模型。 机器学习摒弃了人为向机器输入知识的操作，转而凭借算法自身来学所需的知识。 特征的好坏决定了性能的好坏，人们通过特征工程">
<meta name="keywords" content="cnn,neuralNetwork">
<meta property="og:type" content="article">
<meta property="og:title" content="cnn">
<meta property="og:url" content="http://yoursite.com/2018/07/16/cnn/index.html">
<meta property="og:site_name" content="yOUng&#39;s Blog">
<meta property="og:description" content="本文是阅读魏秀参发布的CNN的记录与思考。 绪论深度学习机器学习：人工智能的一个分支，致力于如何通过计算的手段，利用经验(experience)改善计算系统自身的性能，这里经验往往对应以特征(feature)的形式存储的数据(data)，传统机器学习算法便是通过这些数据产生模型。 机器学习摒弃了人为向机器输入知识的操作，转而凭借算法自身来学所需的知识。 特征的好坏决定了性能的好坏，人们通过特征工程">
<meta property="og:locale" content="en">
<meta property="og:image" content="http://yoursite.com/2018/07/16/cnn/dl.png">
<meta property="og:image" content="http://yoursite.com/2018/07/16/cnn/边缘卷积核.png">
<meta property="og:image" content="http://yoursite.com/2018/07/16/cnn/感受野.png">
<meta property="og:image" content="http://yoursite.com/2018/07/16/cnn/alexnet.png">
<meta property="og:image" content="http://yoursite.com/2018/07/16/cnn/残差网络.png">
<meta property="og:image" content="http://yoursite.com/2018/07/16/cnn/bn.png">
<meta property="og:image" content="http://yoursite.com/2018/07/16/cnn/bn2.png">
<meta property="og:updated_time" content="2018-07-18T23:22:37.403Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="cnn">
<meta name="twitter:description" content="本文是阅读魏秀参发布的CNN的记录与思考。 绪论深度学习机器学习：人工智能的一个分支，致力于如何通过计算的手段，利用经验(experience)改善计算系统自身的性能，这里经验往往对应以特征(feature)的形式存储的数据(data)，传统机器学习算法便是通过这些数据产生模型。 机器学习摒弃了人为向机器输入知识的操作，转而凭借算法自身来学所需的知识。 特征的好坏决定了性能的好坏，人们通过特征工程">
<meta name="twitter:image" content="http://yoursite.com/2018/07/16/cnn/dl.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2018/07/16/cnn/"/>





  <title>cnn | yOUng's Blog</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">yOUng's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/07/16/cnn/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="young">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="yOUng's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">cnn</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-07-16T10:46:39+08:00">
                2018-07-16
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>本文是阅读魏秀参发布的<a href="http://lamda.nju.edu.cn/weixs/book/CNN_book.html" target="_blank" rel="noopener">CNN</a>的记录与思考。</p>
<h1 id="绪论"><a href="#绪论" class="headerlink" title="绪论"></a>绪论</h1><h2 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a>深度学习</h2><p>机器学习：人工智能的一个分支，致力于如何通过计算的手段，利用经验(experience)改善计算系统自身的性能，这里经验往往对应以特征(feature)的形式存储的数据(data)，传统机器学习算法便是通过这些数据产生模型。</p>
<p>机器学习摒弃了人为向机器输入知识的操作，转而凭借算法自身来学所需的知识。</p>
<p>特征的好坏决定了性能的好坏，人们通过特征工程(feature engineering)形式的工程试错性的方式来得到数据特征。人们也尝试将特征学习这一过程也用机器自动”学”出来，这便是”表示学习”(representation learning),”深度学习”便是表示学习的一个经典代表。</p>
<p>深度学习以数据的原始形态(raw data)，经过算法层层抽象，将原始数据抽象为自身任务所需的最终特征表示，最后以特征到任务目标的映射作为结束。</p>
<p>深度学习除了模型学习，还有特征学习、特征抽象等任务模块的参与，借助多层任务模块完成最终学习任务，故称作”深度”学习。</p>
<p><img src="/2018/07/16/cnn/dl.png" alt="关系图"></p>
<p>深度学习的伟大之处在于，真正让研究者或工程师摆脱了复杂的特征工程，从而可以专注于解决更加宏观的关键问题。</p>
<h1 id="卷积神经网络基本部件"><a href="#卷积神经网络基本部件" class="headerlink" title="卷积神经网络基本部件"></a>卷积神经网络基本部件</h1><p>过去解决一个人工智能问题，往往通过分治法将其分解为预处理、特征提取与选择、分类器设计等若干步骤，分布解决子问题时，尽管在子问题上得到最优解，但在子问题上的最优并不意味着就能得到全局问题的最后解。</p>
<p>“端到端”即从元输入到期望输出的映射完全交给模型，有更大可能获得全局最优解。</p>
<h2 id="符号表示"><a href="#符号表示" class="headerlink" title="符号表示"></a>符号表示</h2><p>用三维张量$x^l \in R^{H^l \times W^l \times D^l}$表示卷积神经网络第l蹭的输入，用三元组$(i^l,j^l,d^l)$来指示该张量对应第$i^l$行,第$j^l$列,第$d^l$通道(channel)位置的元素。</p>
<h2 id="卷积"><a href="#卷积" class="headerlink" title="卷积"></a>卷积</h2><p>三维输入时卷及操作实际只是将二维卷积扩展到了对应位置的所有通道上(即$D^l$)，最终将以此卷积处理的所有$HWD^l$个元素求和作为该卷积位置结果。</p>
<h2 id="卷积核的作用"><a href="#卷积核的作用" class="headerlink" title="卷积核的作用"></a>卷积核的作用</h2><p>卷积核作用于局部图像区域获得图像的局部信息。</p>
<p>以下为三种边缘卷积核(亦可称为滤波器)：<br><img src="/2018/07/16/cnn/边缘卷积核.png" alt="滤波器"></p>
<p>$K^e$可检测整体边缘滤波器，若某像素(x,y)处在物体边缘，即四周的像素点与(x,y)会有显著差异，即$k^e$不为0，即检测出边缘。若不在边缘，四周的与像素点与(x,y)应该差不多，此时$k^e$为0。类似$k^h$和$k^v$的横向、纵向边缘滤波器可分别保留横向、纵向的边缘信息。</p>
<p>这些卷积核都是网络学出来的，可以学到任意角度的边缘滤波器，还有检测颜色、形状、纹理等等。</p>
<h2 id="汇合"><a href="#汇合" class="headerlink" title="汇合"></a>汇合</h2><p>汇合包括平均汇合和最大汇合。</p>
<p>不需要参数，仅需要核大小(kernel size)和步长(stride)等超参数。</p>
<p>每一层都有一个汇合核，第$l$层的汇合核可表示为$p^l \in R^(H \times W \times D^l)$</p>
<p>汇合层作用：</p>
<ol>
<li>特征不变形。汇合操作使模型更关注是否存在某些特征而不是特征的具体位置。</li>
<li>特征降维，减小了下一层输入大小，进而减小计算量和参数个数。</li>
</ol>
<h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><p>激活函数的引入为的是增强网络的表达能力(即非线性)。</p>
<h2 id="全连接层"><a href="#全连接层" class="headerlink" title="全连接层"></a>全连接层</h2><p>如果说卷积层、汇合层和激活函数层等操作是将原始数据映射到隐层特征 空间的话，全连接层则起到将学到的特征表示映射到样本的标记空间的作用。</p>
<h1 id="卷积神经网络经典结构"><a href="#卷积神经网络经典结构" class="headerlink" title="卷积神经网络经典结构"></a>卷积神经网络经典结构</h1><h2 id="感受野"><a href="#感受野" class="headerlink" title="感受野"></a>感受野</h2><p><img src="/2018/07/16/cnn/感受野.png" alt="感受野"><br>由图可知，小卷积核可通过多层叠加取得与大卷积核同等规模的感受野。</p>
<p>小卷积核的好处：</p>
<ol>
<li>加深了网络深度进而增强了网络容量和复杂度。</li>
<li>增强网络容量的同时减少了参数的个数。假设卷积核对应的输入输出特征张量的深度均为C，则$7\times7$的卷积核对应参数有$C\times(7\times7\times C)=49C^2$个，$3\times3$卷积核仅需要$3\times C\times(3\times3\times C)=27C^2$个。</li>
</ol>
<h2 id="分布式表示"><a href="#分布式表示" class="headerlink" title="分布式表示"></a>分布式表示</h2><p>通过实验证明，对于某个模式，如鸟的躯干，会有不同的卷积核(其实就是神经元)产生响应，对于某个卷积核，会在不同模式上产生响应，如躯干和头部。</p>
<p>除了分布式表示特性，神经网络响应多呈现”稀疏”(sparse)特性，即响应区域集中占原图比例较小。</p>
<h2 id="深度特征的层次性"><a href="#深度特征的层次性" class="headerlink" title="深度特征的层次性"></a>深度特征的层次性</h2><p>卷积操作可获取图像区域不同类型特征，汇合等操作是对特征进行融合和抽象。</p>
<p>随着卷积核汇合等操作的堆叠，各层得到的深度特征逐渐从泛化特征(边缘、纹理)过度到高层语义表示(躯干、头部)。</p>
<h2 id="经典网络之Alex-Net"><a href="#经典网络之Alex-Net" class="headerlink" title="经典网络之Alex-Net"></a>经典网络之Alex-Net</h2><p>Alex-Net于2012年ImageNet竞赛中取得冠军。</p>
<p>模型：<img src="/2018/07/16/cnn/alexnet.png" alt="alexnet"></p>
<p>在网络结构和基本操作方面，Alex-Net改进很小，仅在网络深度和复杂度上游较大优势。</p>
<p>其他贡献：</p>
<ol>
<li>首先将cnn用于cv领域。</li>
<li>利用gpu进行网络训练。</li>
<li>提出了一些trick，比如使用relu，dropout(随机失活)，还有局部响应规范化(LRN)，要求对相同空间位置上相邻深度的卷积结果做规范化。</li>
</ol>
<h2 id="经典网络之残差网络模型"><a href="#经典网络之残差网络模型" class="headerlink" title="经典网络之残差网络模型"></a>经典网络之残差网络模型</h2><p>公式可表示为: $y=F(x,w)+x$，做简单变换后，可得: $F(x,w)=y-x$，残差项为$y-x$，称为残差函数。</p>
<p>模型：<img src="/2018/07/16/cnn/残差网络.png" alt="残差网络"></p>
<p>通过这种近路连接的方式，即使面对特别深的网络，也可以通过反向传播端到端学习。</p>
<p>残差网络以全局平均汇合从代替了全连接层，一方面使得参数减少，一方面减少了过拟合的风险。</p>
<h1 id="卷积神经网络的压缩"><a href="#卷积神经网络的压缩" class="headerlink" title="卷积神经网络的压缩"></a>卷积神经网络的压缩</h1><p>神经网络面临严峻的过参数化(over-parameterization)。</p>
<p>总体而言，绝大多数的压缩算法，均是将庞大而复杂的预训练模型(pre-trained model)转为一个精简的小模型。</p>
<p>压缩技术分为”前端压缩”和”后端压缩”。</p>
<p>前端压缩：不改变原网络结构的压缩技术。主要包括知识蒸馏、紧凑的模型结构设计、滤波器层面的剪枝。</p>
<p>后端压缩：尽可能减少模型的大小，不得不对原有的网络结构进行改造，这样的改造往往不可逆。主要包括低秩近似、未加限制的剪枝、参数量化以及二值网络等。</p>
<h2 id="低秩近似"><a href="#低秩近似" class="headerlink" title="低秩近似"></a>低秩近似</h2><p>对卷积过程所需要的权重矩阵(通常稠密且巨大)进行重构，具体方法包括结构化矩阵，使用局矩阵分解来降低权重矩阵的参数。</p>
<p>低秩近似适用于中小型网络，主要是由于其超参数量与网络层数呈线性变化趋势，随着网络层数的增加与模型复杂度的提升，搜索空间会急剧增大。</p>
<h2 id="剪枝与稀疏约束"><a href="#剪枝与稀疏约束" class="headerlink" title="剪枝与稀疏约束"></a>剪枝与稀疏约束</h2><p>剪枝的好处在于能够减小模型的复杂度，还能防止过拟合。</p>
<p>常用的剪枝流程如下：衡量神经元(粒度不同，可以是个权重连接，也可以是整个滤波器)的重要程度，移掉不重要的神经元，对网络进行微调，然后进行下一轮的剪枝。</p>
<p>直接剪权重连接不足之处是剪枝后的网络是非结构化的，需要运行专门的库来运行模型，严重制约了通用性。</p>
<p>剪滤波器的做法之一是根据滤波器权重的绝对值作为其权值，不足是很多情况下，小权值的滤波器对损失函数也能起到重要的影响。</p>
<p>剪滤波器还有一种方案是由数据驱动剪枝，根据网络输出的通道的稀疏程度来判断滤波器的作用，如果一个滤波器输出几乎全部为0，则该滤波器是多余的。但该方法仍是启发式算法， 只能根据实验结果来评价其好坏。</p>
<p>利用稀疏约束对网络进行剪枝也是一个重要的方向(该部分暂时还不是太明白)。</p>
<p>连接级别的剪枝太细，滤波器级别的剪枝太粗。有人提出了结构化系数训练的方法以滤波器、通道、网络深度作为约束对象，将其加到损失函数的约束项中。</p>
<p>剪枝的关键点是如何衡量个别权重对整体模型的重要程度，尤其是对于深度学习而言，几乎不可能从理论上确保某一选择策略是最优的。另一方面，由于剪枝操作对网络结构的破坏程度极小，这种良好的特性往往被当做网络压缩过程的前端处理。将剪枝与其他后端压缩技术相结合，能够达到网络模型的最大程度压缩。</p>
<h2 id="参数量化"><a href="#参数量化" class="headerlink" title="参数量化"></a>参数量化</h2><p>最简单的一种量化算法便是标量量化(scalar quantization)，将权重矩阵转化为向量$w \in R^{1 \times mn}$，然后对权重向量的元素进行k个簇的聚类，记录与码本(codebook)之中，原权重矩阵只负责记录各自聚类中心在码本中的索引。</p>
<h2 id="知识蒸馏"><a href="#知识蒸馏" class="headerlink" title="知识蒸馏"></a>知识蒸馏</h2><p>迁移学习的一种，目的是将一个庞大而复杂的模型所学到的知识，通过一定手段迁移到精简的小模型上，使小模型能够获得和大模型相似的性能。</p>
<p>这里主要包括两个问题，如何提取知识，如何完成迁移。</p>
<p>Jimmy等人认为，softmax层的输入与类别标签相比，包含了更丰富的监督信息，所以他们在实验中选择浅层的小模型来”模仿”深层的大模型，但仍需要很多参数。</p>
<p>Hinton等人认为Softmax层会是更好的选择，可被认为一种”软标签”即不仅包括类别概率，而且还包含了不同类别之间的相似信息，为了更好的获得”软标签”，又引入了温度$T$这一变量来控制平滑程度。该做法不足之处在于，$T$不确定，维度较高时，模型难以收敛。</p>
<p>Luo等人认为Softmax的前一层的输出包含更多的信息，这是因为softmax以此为基础进行计算，但同样包含很多噪声和无关信息，他们设计了一个算法来对这些神经元进行选择，主要思想是，保留那些满足如下两点要求的特征维度:一是该维度的特征须具有足够 强的区分度，二是不同维度之间的相关性须尽可能低。</p>
<p>知识蒸馏目前的效果相对主流的剪枝、量化等技术相比，仍存在一定的差距。</p>
<p>知识蒸馏的主要思想是，小模型相比于大模型可以获得更多的监督信息，大模型只是使用标签数据，小模型可以使用标签数据和大模型的输出，进而获得更好的训练效果。</p>
<h1 id="实践应用"><a href="#实践应用" class="headerlink" title="实践应用"></a>实践应用</h1><h2 id="数据扩充"><a href="#数据扩充" class="headerlink" title="数据扩充"></a>数据扩充</h2><ol>
<li>简单的扩充方式：对图像水平翻转，随机抠取，尺度变换，旋转。</li>
<li>Fancy PCA：对图片的RGB像素值做PCA，得到特征向量和特征值，然后根据这些计算一个扰动值，加入到原像素值中。</li>
<li>监督式数据扩充：首先根据原数据训练一个分类的初始模型，利用该模型生成对应的特征图(可知识图像区域与场景标记间的相关概率)，然后扣取相关概率较大的区域扩充数据。</li>
</ol>
<p>这里思考NLP领域为什么关于数据扩充的方式那么少，这个<a href="https://blog.csdn.net/weixin_39312449/article/details/80523518" target="_blank" rel="noopener">博客</a>我觉得说的很有道理。</p>
<h2 id="目标函数"><a href="#目标函数" class="headerlink" title="目标函数"></a>目标函数</h2><h2 id="交叉熵"><a href="#交叉熵" class="headerlink" title="交叉熵"></a>交叉熵</h2><p>这里说一下交叉熵的来源</p>
<p>说交叉熵之前先看看信息量，信息量必须满足两个条件：</p>
<ol>
<li>越小概率的事件产生的信息量越大=&gt; 与概率负相关</li>
<li>两个独立事件同时发生所带来的信息量是独立发生打来信息量的和=&gt;有对数</li>
</ol>
<p>信息量是对一个具体事件发生所带来的信息，熵是在结果出来之前对所有可能产生的信息量的期望。</p>
<p>即$H(X)=-\sum^n_{i=1}p(x_i)logp(x_i)$</p>
<p>相对熵(KL散度)，来衡量两个分布的差异<script type="math/tex">D_{KL}(p||q) = \sum^n_{i=1}p(x_i)log(p(x_i)/q(x_i))$，$D_KL</script>越小，表示q分布和p分布越接近。</p>
<p>衡量label与predicts之间的差距，使用KL散度距离刚好由于KL散度<script type="math/tex">D_{KL}(p||q) = \sum^n_{i=1}p(x_i)log(p(x_i))) - \sum^n_{i=1}p(x_i)log(q(x_i))</script>，前一部分是常量，只需关注后一部分即交叉熵即可。</p>
<h2 id="网络正则化"><a href="#网络正则化" class="headerlink" title="网络正则化"></a>网络正则化</h2><p>防止过拟合，提升泛化能力。</p>
<ol>
<li>$l_1$，$l_2$。$l_2$正则化效果好于$l_1$，$l_1$可求得稀疏解，二者可联合使用，此时被称为”Elastic”网络正则化。</li>
<li>最大范数约束是通过参数量级的范数设置上限进而进行正则化，可防止”梯度爆炸”。</li>
<li>dropout(随机失活)。加了dropout之后相当于训练阶段训练了几个子网络，测试相当于子网络的平均集成。</li>
<li>验证集。验证集准确率一直低于训练集上的准确率，但无明显下降趋势。这说明此时模型复杂度欠缺，模型表示能力有限——属“欠拟合”状态。若验证集曲线不仅低于训练集，且随着训练轮 数增长有明显下降趋势，说明模型已经过拟合。</li>
<li>加入额外的训练数据。</li>
</ol>
<h2 id="超参数设定和网络训练"><a href="#超参数设定和网络训练" class="headerlink" title="超参数设定和网络训练"></a>超参数设定和网络训练</h2><h2 id="批规范化操作"><a href="#批规范化操作" class="headerlink" title="批规范化操作"></a>批规范化操作</h2><p>Google提出的批规范化操作(batch normalization，简称BN)，可以缓解梯度消失的问题。算法如图所示：<br><img src="/2018/07/16/cnn/bn.png" alt="BN"></p>
<p>前三步操作中规中矩，使得x的均值为0，方差为1，有趣的在第四步，尺度变化和偏移，加这一步的目的是：BN可以看做是原模型上的新操作，这个新操作也就会改变x(当然也有可能不改变)，最后一布的目的是可以还原原始的输入(模型自主的去学$\beta$和$\gamma$)，如此一来网络的容量(capacity)便会提升。</p>
<p>统计机器学习中的一个经典假设是”源空间”和”目标空间”的数据分布式一致的，BN的作用就是规范化输入(固定每层的输入信号的均值和方差)。</p>
<p>以下的图片(来自:<a href="https://v.youku.com/v_show/id_XMTg1MTYwNDg2OA==" target="_blank" rel="noopener">莫烦的视频</a>)对BN解决梯度消失问题做了很直观的说明：<br><img src="/2018/07/16/cnn/bn2.png" alt="BN对梯度消失问题的解决">]</p>
<p>BN一般用在非线性映射函数前，若神经网络训练时收敛速度较慢，或”梯度爆炸”等无法训练的状况发生时也可以尝试用BN解决(解决梯度爆炸怎么说…)。</p>
<h2 id="网络模型优化算法选择"><a href="#网络模型优化算法选择" class="headerlink" title="网络模型优化算法选择"></a>网络模型优化算法选择</h2>
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/cnn/" rel="tag"># cnn</a>
          
            <a href="/tags/neuralNetwork/" rel="tag"># neuralNetwork</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/06/13/hello-world/" rel="next" title="Hello World">
                <i class="fa fa-chevron-left"></i> Hello World
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/11/02/Zero-Shot-Learning/" rel="prev" title="Zero-Shot Learning">
                Zero-Shot Learning <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">young</p>
              <p class="site-description motion-element" itemprop="description">Yawen Ouyang's Blog</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">4</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">4</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/yawenouyang" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#绪论"><span class="nav-number">1.</span> <span class="nav-text">绪论</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#深度学习"><span class="nav-number">1.1.</span> <span class="nav-text">深度学习</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#卷积神经网络基本部件"><span class="nav-number">2.</span> <span class="nav-text">卷积神经网络基本部件</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#符号表示"><span class="nav-number">2.1.</span> <span class="nav-text">符号表示</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#卷积"><span class="nav-number">2.2.</span> <span class="nav-text">卷积</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#卷积核的作用"><span class="nav-number">2.3.</span> <span class="nav-text">卷积核的作用</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#汇合"><span class="nav-number">2.4.</span> <span class="nav-text">汇合</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#激活函数"><span class="nav-number">2.5.</span> <span class="nav-text">激活函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#全连接层"><span class="nav-number">2.6.</span> <span class="nav-text">全连接层</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#卷积神经网络经典结构"><span class="nav-number">3.</span> <span class="nav-text">卷积神经网络经典结构</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#感受野"><span class="nav-number">3.1.</span> <span class="nav-text">感受野</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#分布式表示"><span class="nav-number">3.2.</span> <span class="nav-text">分布式表示</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#深度特征的层次性"><span class="nav-number">3.3.</span> <span class="nav-text">深度特征的层次性</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#经典网络之Alex-Net"><span class="nav-number">3.4.</span> <span class="nav-text">经典网络之Alex-Net</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#经典网络之残差网络模型"><span class="nav-number">3.5.</span> <span class="nav-text">经典网络之残差网络模型</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#卷积神经网络的压缩"><span class="nav-number">4.</span> <span class="nav-text">卷积神经网络的压缩</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#低秩近似"><span class="nav-number">4.1.</span> <span class="nav-text">低秩近似</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#剪枝与稀疏约束"><span class="nav-number">4.2.</span> <span class="nav-text">剪枝与稀疏约束</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参数量化"><span class="nav-number">4.3.</span> <span class="nav-text">参数量化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#知识蒸馏"><span class="nav-number">4.4.</span> <span class="nav-text">知识蒸馏</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#实践应用"><span class="nav-number">5.</span> <span class="nav-text">实践应用</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#数据扩充"><span class="nav-number">5.1.</span> <span class="nav-text">数据扩充</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#目标函数"><span class="nav-number">5.2.</span> <span class="nav-text">目标函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#交叉熵"><span class="nav-number">5.3.</span> <span class="nav-text">交叉熵</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#网络正则化"><span class="nav-number">5.4.</span> <span class="nav-text">网络正则化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#超参数设定和网络训练"><span class="nav-number">5.5.</span> <span class="nav-text">超参数设定和网络训练</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#批规范化操作"><span class="nav-number">5.6.</span> <span class="nav-text">批规范化操作</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#网络模型优化算法选择"><span class="nav-number">5.7.</span> <span class="nav-text">网络模型优化算法选择</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2018 &mdash; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">young</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
